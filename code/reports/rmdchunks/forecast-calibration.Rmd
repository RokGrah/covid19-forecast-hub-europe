<!--- 
- Table with coverage by horizon for this week
- Plot of coverage by target and horizon over time
- PIT histograms
--->
# Forecast calibration

The table and plot below show this week's _coverage_ of the ensemble model at the 50% and 95% level, across the 32 countries. This shows the proportion of observations that fall within a given prediction interval. Ideally, a forecast model would achieve 50% coverage of 0.50 (i.e., 50% of observations fall within the 50% prediction interval) and 95% coverage of 0.95 (i.e., 95% of observations fall within the 95% prediction interval). Values of coverage greater than these nominal values indicate that the forecasts are _underconfident_, i.e. prediction intervals tend to be too wide, whereas values of coverage smaller than these nominal values indicate that the ensemble forecasts are _overconfident_, i.e. prediction intervals tend to be too narrow.

## Overall coverage

```{r coverage, echo = FALSE, include = include_calibration, results = 'asis'}
scores <- scoringutils::eval_forecasts(
  data,
  summarise_by = c("model", "range", "quantile",
                  "target_variable", "horizon"),
  pit_plots = TRUE
)

if (nrow(scores) > 0) {
coverage <- scores %>%
  dplyr::filter(range %in% c(50, 95)) %>%
  select(range, `Target variable` = target_variable,
         `Forecast horizon` = horizon, coverage) %>%
  distinct() %>%
  mutate(range = paste0(range, "% coverage"),
         `Forecast horizon` =
           paste0(`Forecast horizon`, " week",
                  if_else(`Forecast horizon` == 1, "", "s")),
         `Target variable` = recode_factor(`Target variable`,
                                         `inc case` = "Cases",
                                         `inc death` = "Deaths"),
         coverage = round(coverage, 2))

coverage %>%
  tidyr::pivot_wider(names_from = range, values_from = coverage) %>%
  group_by(`Forecast horizon`) %>%
  DT::datatable(extensions = c('FixedColumns', 'Buttons'),
                width = "100%",
                options = list(
                  paging = FALSE,
                  info = FALSE,
                  buttons = c('csv', 'excel'),
                  dom = 'Bfrtip',
                  scrollX = TRUE
                ),
                class = 'white-space: nowrap') %>%
  htmltools::tagList()
} else {
  cat("No coverage figures shown as now 50%/90% predictive intervals are available.\n")
}
```

## PIT histograms

```{r pit_width, echo = FALSE}
width <- 0.1
```

The figures below are _PIT histograms_ for the all past forecasts. These show the proportion of true values within each predictive quantile (width: `r width`). If the forecasts were perfectly calibrated, observations would fall evenly across these equally-spaced quantiles, i.e. the histograms would be flat.

```{r pit, echo = FALSE, results = 'asis', include = include_calibration}
quantiles <- seq(width, 1 - width, by = width)

even_quantiles <- scores %>%
  filter(!is.na(quantile) & round(quantile, 3) %in% round(quantiles, 3))

if (nrow(even_quantiles) > 0) {
pit <- even_quantiles %>%
  mutate(horizon = paste0(horizon, " week", if_else(horizon == 1, "", "s"))) %>%
  arrange(target_variable, horizon, quantile) %>%
  group_by(target_variable, horizon) %>%
  summarise(quantile = c(quantile, 1),
            pit_bin = diff(c(0, quantile_coverage, 1)))

  p <- ggplot(pit, aes(x = quantile - width / 2, y = pit_bin)) +
    geom_col() +
    theme_light() +
    facet_grid(horizon ~ target_variable) +
    xlab("Quantile") + ylab("Proportion") +
    geom_hline(yintercept = width, linetype = "dashed")

  print(p)
} else {
  cat("No PIT histogram shown because not all 23 predictive quantiles are available.\n")
}
```
